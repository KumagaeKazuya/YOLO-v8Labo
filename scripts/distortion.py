import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import time
from ultralytics import YOLO
from collections import defaultdict, deque
import logging
import csv
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
from enum import Enum

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

class PhoneUsageState(Enum):
    """ã‚¹ãƒãƒ›ä½¿ç”¨çŠ¶æ…‹ã®è©³ç´°åˆ†é¡"""
    NOT_USING = "not_using"
    HOLDING_NEAR_FACE = "holding_near_face"
    LOOKING_DOWN = "looking_down"
    BOTH_HANDS_UP = "both_hands_up"
    UNCERTAIN = "uncertain"
    TRANSITIONING = "transitioning"

class PersonOrientation(Enum):
    """äººç‰©ã®å‘ãã®åˆ†é¡"""
    FRONT_FACING = "front_facing"
    BACK_FACING = "back_facing"
    SIDE_FACING = "side_facing"
    UNCERTAIN = "uncertain"

@dataclass
class PostureFeatures:
    """å§¿å‹¢ç‰¹å¾´é‡ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    head_angle: float
    hand_face_distance_left: float
    hand_face_distance_right: float
    shoulder_hand_angle_left: float
    shoulder_hand_angle_right: float
    head_tilt: float
    neck_forward: float
    confidence_score: float
    visible_keypoints: int
    orientation: PersonOrientation = PersonOrientation.UNCERTAIN

@dataclass
class DetectionResult:
    """æ¤œå‡ºçµæœã®è©³ç´°æƒ…å ±"""
    frame_id: int
    track_id: int
    timestamp: float
    phone_state: PhoneUsageState
    confidence: float
    features: PostureFeatures
    bbox: Tuple[int, int, int, int]
    grid_position: Tuple[int, int]
    keypoints_visible: List[bool]
    orientation: PersonOrientation


class EnhancedCSVLogger:
    """æ©Ÿæ¢°å­¦ç¿’ç”¨æ‹¡å¼µCSVãƒ­ã‚¬ãƒ¼"""

    def __init__(self, csv_path="enhanced_detection_log.csv"):
        self.csv_path = csv_path
        self.csv_file = None
        self.csv_writer = None
        self.start_time = time.time()
        self.prev_keypoints = {}  # track_id -> previous keypoints for motion calculation
        self.log_count = 0

        # CSV ãƒ˜ãƒƒãƒ€ãƒ¼å®šç¾©
        self.headers = [
            # åŸºæœ¬æƒ…å ±
            "timestamp", "frame_idx", "relative_time_sec", "frame_interval_ms",

            # äººç‰©è­˜åˆ¥æƒ…å ±
            "person_id", "track_confidence", "detection_confidence",

            # ä½ç½®æƒ…å ±
            "bbox_x1", "bbox_y1", "bbox_x2", "bbox_y2",
            "bbox_width", "bbox_height", "bbox_area",
            "center_x", "center_y", "grid_row", "grid_col",

            # è©³ç´°è¡Œå‹•åˆ¤å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            "phone_state", "phone_state_confidence", "person_orientation",
            "phone_detection_method",  # "front_view", "back_view", "failed"

            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåº§æ¨™ (17ç‚¹ x 3åº§æ¨™ = 51åˆ—)
            *[f"kp_{i}_{coord}" for i in range(17) for coord in ["x", "y", "conf"]],

            # é«˜åº¦ãªå‹•ä½œç‰¹å¾´é‡
            "head_angle", "hand_face_distance_left", "hand_face_distance_right",
            "shoulder_hand_angle_left", "shoulder_hand_angle_right",
            "head_tilt", "neck_forward", "movement_speed", "pose_change_magnitude",

            # å§¿å‹¢åˆ†æï¼ˆè©³ç´°ï¼‰
            "shoulder_width", "torso_lean_angle", "arm_symmetry",
            "posture_stability", "attention_direction",

            # ç”»åƒå“è³ªãƒ»ç’°å¢ƒè¦å› 
            "frame_brightness", "frame_contrast", "blur_score", "noise_level",

            # æ™‚ç³»åˆ—ç‰¹å¾´
            "consecutive_phone_frames", "phone_state_duration_sec",
            "position_stability", "tracking_quality",

            # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨
            "manual_label_phone", "manual_label_posture", "manual_label_attention",
            "annotation_confidence", "annotator_id", "review_required",

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "video_source", "processing_version", "model_version", "notes"
        ]

        self.init_csv()

    def init_csv(self):
        """CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–"""
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)

            self.csv_file = open(self.csv_path, 'w', newline='', encoding='utf-8')
            self.csv_writer = csv.writer(self.csv_file)
            self.csv_writer.writerow(self.headers)

            # å³åº§ã«ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã¿
            self.csv_file.flush()

            logger.info(f"æ‹¡å¼µCSVãƒ­ã‚°ã‚’åˆæœŸåŒ–: {self.csv_path}")
            logger.info(f"CSVãƒ˜ãƒƒãƒ€ãƒ¼: {self.headers}")

        except Exception as e:
            logger.error(f"CSVåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def log_detection_result(self, detection_result: DetectionResult, frame,
                        keypoints, grid_row, grid_col, video_source="unknown"):
        """DetectionResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ‹¡å¼µCSVã«ãƒ­ã‚°ã‚’è¨˜éŒ²"""
        try:
            self.log_count += 1
            current_time = time.time()
            timestamp = datetime.now().isoformat()
            relative_time = current_time - self.start_time

            # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æƒ…å ±
            x1, y1, x2, y2 = detection_result.bbox
            bbox_width = x2 - x1
            bbox_height = y2 - y1
            bbox_area = bbox_width * bbox_height
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2

            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåº§æ¨™ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
            kp_coords = []
            for i in range(17):
                if i < len(keypoints):
                    x = float(keypoints[i][0]) if len(keypoints[i]) > 0 else 0.0
                    y = float(keypoints[i][1]) if len(keypoints[i]) > 1 else 0.0
                    conf = float(keypoints[i][2]) if len(keypoints[i]) > 2 else 0.0
                    kp_coords.extend([x, y, conf])
                else:
                    kp_coords.extend([0.0, 0.0, 0.0])

            # ç”»åƒå“è³ªè¨ˆç®—
            quality_metrics = self.calculate_image_quality(frame, detection_result.bbox)

            # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™
            log_data = [
                # åŸºæœ¬æƒ…å ±
                timestamp, detection_result.frame_id, relative_time, 33.33,

                # äººç‰©è­˜åˆ¥æƒ…å ±
                detection_result.track_id, 0.9, detection_result.confidence,

                # ä½ç½®æƒ…å ±
                float(x1), float(y1), float(x2), float(y2),
                float(bbox_width), float(bbox_height), float(bbox_area),
                float(center_x), float(center_y), grid_row, grid_col,

                # è©³ç´°è¡Œå‹•åˆ¤å®š
                detection_result.phone_state.value, detection_result.confidence,
                detection_result.orientation.value, "advanced_detection",

                # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåº§æ¨™ï¼ˆ51åˆ—ï¼‰
                *kp_coords,

                # é«˜åº¦ãªå‹•ä½œç‰¹å¾´é‡
                detection_result.features.head_angle,
                detection_result.features.hand_face_distance_left,
                detection_result.features.hand_face_distance_right,
                detection_result.features.shoulder_hand_angle_left,
                detection_result.features.shoulder_hand_angle_right,
                detection_result.features.head_tilt,
                detection_result.features.neck_forward,
                0.0, 0.0,  # movement_speed, pose_change_magnitude

                # å§¿å‹¢åˆ†æï¼ˆè©³ç´°ï¼‰
                100.0, 0.0, 0.8, 0.9, "forward",  # shoulder_widthç­‰

                # ç”»åƒå“è³ª
                quality_metrics['brightness'], quality_metrics['contrast'],
                quality_metrics['blur_score'], quality_metrics['noise_level'],

                # æ™‚ç³»åˆ—ç‰¹å¾´ï¼ˆä»®å€¤ï¼‰
                0, 0.0, 0.9, detection_result.features.confidence_score,

                # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼ˆç©ºæ¬„ï¼‰
                "", "", "", 0.0, "", False,

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                video_source, "v2.0", "yolo11x-pose-advanced", ""
            ]

            # ãƒ‡ãƒ¼ã‚¿é•·ãƒã‚§ãƒƒã‚¯
            if len(log_data) != len(self.headers):
                logger.error(f"ãƒ‡ãƒ¼ã‚¿é•·ä¸ä¸€è‡´: æœŸå¾…{len(self.headers)}, å®Ÿéš›{len(log_data)}")
                while len(log_data) < len(self.headers):
                    log_data.append("")
                log_data = log_data[:len(self.headers)]

            # CSVæ›¸ãè¾¼ã¿
            if self.csv_writer:
                self.csv_writer.writerow(log_data)
                self.csv_file.flush()

                if self.log_count % 30 == 0:
                    logger.info(f"æ‹¡å¼µCSVè¨˜éŒ²ç¶™ç¶š: {self.log_count}è¡Œç›®")

        except Exception as e:
            logger.error(f"æ‹¡å¼µãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def calculate_image_quality(self, frame, bbox):
        """ç”»åƒå“è³ªæŒ‡æ¨™ã‚’è¨ˆç®—"""
        try:
            x1, y1, x2, y2 = map(int, bbox)
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)

            if x2 <= x1 or y2 <= y1:
                return {'brightness': 0, 'contrast': 0, 'blur_score': 0, 'noise_level': 0}

            roi = frame[y1:y2, x1:x2]
            if roi.size == 0:
                return {'brightness': 0, 'contrast': 0, 'blur_score': 0, 'noise_level': 0}

            gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) > 2 else roi

            brightness = float(np.mean(gray_roi))
            contrast = float(np.std(gray_roi))
            blur_score = float(cv2.Laplacian(gray_roi, cv2.CV_64F).var())
            noise_level = 0.0

            return {
                'brightness': brightness,
                'contrast': contrast,
                'blur_score': blur_score,
                'noise_level': noise_level
            }
        except Exception as e:
            logger.warning(f"ç”»åƒå“è³ªè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return {'brightness': 0, 'contrast': 0, 'blur_score': 0, 'noise_level': 0}

    def close(self):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒ­ãƒ¼ã‚º"""
        try:
            if self.csv_file:
                self.csv_file.flush()
                self.csv_file.close()
                logger.info(f"æ‹¡å¼µCSVãƒ­ã‚°ä¿å­˜å®Œäº†: {self.csv_path}")
                logger.info(f"ç·è¨˜éŒ²æ•°: {self.log_count}è¡Œ")
        except Exception as e:
            logger.error(f"CSVã‚¯ãƒ­ãƒ¼ã‚ºã‚¨ãƒ©ãƒ¼: {e}")

class VideoDistortionCorrector:
    """å‹•ç”»ã®æ­ªã¿è£œæ­£ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹è‰¯ç‰ˆ - yolo_checker.pyæº–æ‹ ï¼‰"""

    def __init__(self, k1=-0.1, k2=0.0, p1=0.0, p2=0.0, k3=0.0, alpha=0.6, focal_scale=0.9):
        """
        æ­ªã¿è£œæ­£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–

        Args:
            k1, k2, k3: æ”¾å°„æ­ªã¿ä¿‚æ•°
            p1, p2: æ¥ç·šæ­ªã¿ä¿‚æ•°
            alpha: æ–°ã—ã„ã‚«ãƒ¡ãƒ©ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            focal_scale: ç„¦ç‚¹è·é›¢ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        """
        self.k1 = k1
        self.k2 = k2
        self.p1 = p1
        self.p2 = p2
        self.k3 = k3
        self.alpha = alpha
        self.focal_scale = focal_scale
        self.map_x = None
        self.map_y = None
        self.original_camera_matrix = None
        self.new_camera_matrix = None
        self.dist_coeffs = None

        logger.info(f"æ­ªã¿è£œæ­£åˆæœŸåŒ–ï¼ˆæ”¹è‰¯ç‰ˆï¼‰:")
        logger.info(f"  k1={k1}, k2={k2}, k3={k3}")
        logger.info(f"  p1={p1}, p2={p2}")
        logger.info(f"  alpha={alpha}, focal_scale={focal_scale}")

    def create_correction_maps(self, width, height):
        """æ­ªã¿è£œæ­£ç”¨ã®ãƒãƒƒãƒ—ã‚’ä½œæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        logger.info(f"é«˜ç²¾åº¦æ­ªã¿è£œæ­£ãƒãƒƒãƒ—ä½œæˆé–‹å§‹: {width}x{height}")

        # ã‚«ãƒ¡ãƒ©å†…éƒ¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        fx = fy = width * self.focal_scale
        cx, cy = width / 2.0, height / 2.0

        self.original_camera_matrix = np.array(
            [[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32
        )

        # 5ã¤ã®æ­ªã¿ä¿‚æ•°ã‚’ä½¿ç”¨ï¼ˆé«˜ç²¾åº¦è£œæ­£ï¼‰
        self.dist_coeffs = np.array(
            [self.k1, self.k2, self.p1, self.p2, self.k3], dtype=np.float32
        )

        # æœ€é©ãªæ–°ã—ã„ã‚«ãƒ¡ãƒ©ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
        self.new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(
            self.original_camera_matrix,
            self.dist_coeffs,
            (width, height),
            self.alpha,
            (width, height),
        )

        # è£œæ­£ãƒãƒƒãƒ—ä½œæˆ
        self.map_x, self.map_y = cv2.initUndistortRectifyMap(
            self.original_camera_matrix,
            self.dist_coeffs,
            None,
            self.new_camera_matrix,
            (width, height),
            cv2.CV_32FC1,
        )

        logger.info("é«˜ç²¾åº¦æ­ªã¿è£œæ­£ãƒãƒƒãƒ—ä½œæˆå®Œäº†")
        self._log_map_statistics()

    def _log_map_statistics(self):
        """ãƒãƒƒãƒ—ã®çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        if self.map_x is not None and self.map_y is not None:
            x_mean, x_std = np.mean(self.map_x), np.std(self.map_x)
            y_mean, y_std = np.mean(self.map_y), np.std(self.map_y)
            logger.info(f"è£œæ­£ãƒãƒƒãƒ—çµ±è¨ˆ: X(å¹³å‡={x_mean:.2f}, æ¨™æº–åå·®={x_std:.2f}), Y(å¹³å‡={y_mean:.2f}, æ¨™æº–åå·®={y_std:.2f})")

    def apply_correction(self, frame):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ­ªã¿è£œæ­£ã‚’é©ç”¨"""
        if self.map_x is None or self.map_y is None:
            logger.warning("è£œæ­£ãƒãƒƒãƒ—ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return frame

        return cv2.remap(
            frame, self.map_x, self.map_y, cv2.INTER_LINEAR,
            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0)
        )

class OrderedIDTracker:
    """å·¦ã‹ã‚‰é †ã«IDã‚’å‰²ã‚ŠæŒ¯ã‚‹è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""

    def __init__(self, distance_threshold=100, max_missing_frames=30):
        self.distance_threshold = distance_threshold
        self.max_missing_frames = max_missing_frames
        self.tracked_persons = {}
        self.next_id = 1

    def update_tracks(self, detections):
        """æ¤œå‡ºçµæœã‚’æ›´æ–°ã—ã€å·¦ã‹ã‚‰é †ã«IDã‚’å‰²ã‚ŠæŒ¯ã‚‹"""
        if not detections:
            self._update_missing_counts()
            return []

        detections_sorted = sorted(detections, key=lambda d: d['center'][0])
        existing_tracks = sorted(self.tracked_persons.items(), key=lambda t: t[1]['center'][0])

        assigned_detections = []
        used_track_ids = set()

        for detection in detections_sorted:
            best_match_id = None
            best_distance = float('inf')
            detection_center = detection['center']

            for track_id, track_data in existing_tracks:
                if track_id in used_track_ids:
                    continue

                track_center = track_data['center']
                distance = np.linalg.norm(np.array(detection_center) - np.array(track_center))

                if distance < self.distance_threshold and distance < best_distance:
                    best_distance = distance
                    best_match_id = track_id

            if best_match_id is not None:
                assigned_id = best_match_id
                used_track_ids.add(assigned_id)
            else:
                assigned_id = self._get_next_available_id()

            self.tracked_persons[assigned_id] = {
                'center': detection_center,
                'missing_count': 0,
                'bbox': detection['bbox']
            }

            detection_with_id = detection.copy()
            detection_with_id['track_id'] = assigned_id
            assigned_detections.append(detection_with_id)

        # æœªä½¿ç”¨ã®è¿½è·¡å¯¾è±¡ã‚’æ›´æ–°
        for track_id in list(self.tracked_persons.keys()):
            if track_id not in used_track_ids:
                self.tracked_persons[track_id]['missing_count'] += 1
                if self.tracked_persons[track_id]['missing_count'] > self.max_missing_frames:
                    del self.tracked_persons[track_id]

        return assigned_detections

    def _get_next_available_id(self):
        """æ¬¡ã«åˆ©ç”¨å¯èƒ½ãªIDã‚’å–å¾—"""
        existing_ids = set(self.tracked_persons.keys())
        for i in range(1, max(existing_ids) + 2 if existing_ids else 2):
            if i not in existing_ids:
                return i
        return max(existing_ids) + 1 if existing_ids else 1

    def _update_missing_counts(self):
        """å…¨ã¦ã®è¿½è·¡å¯¾è±¡ã®missing_countã‚’æ›´æ–°"""
        for track_id in list(self.tracked_persons.keys()):
            self.tracked_persons[track_id]['missing_count'] += 1
            if self.tracked_persons[track_id]['missing_count'] > self.max_missing_frames:
                del self.tracked_persons[track_id]

    def get_active_tracks(self):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªè¿½è·¡å¯¾è±¡ã‚’å–å¾—"""
        return {tid: data for tid, data in self.tracked_persons.items()}

class AdvancedPostureDetectionSystem:
    """é«˜åº¦ãªå§¿å‹¢æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆyolo_checker.pyæº–æ‹ ï¼‰"""

    def __init__(self, model_path="models/yolo11x-pose.pt"):
        self.model = YOLO(model_path)
        self.id_tracker = OrderedIDTracker(distance_threshold=100, max_missing_frames=30)
        self.person_states = {}

        # åŸºæœ¬è¨­å®š
        self.config = {
            "conf_threshold": 0.4,
            "phone_distance_threshold": 100,
            "head_angle_threshold": 30,
            "neck_forward_threshold": 0.2,
        }

        # å¾Œã‚å‘ãç”¨ã®è¨­å®š
        self.back_view_config = {
            "hand_head_distance_threshold": 150,
            "arm_bend_threshold": 90,
            "shoulder_tilt_threshold": 15,
            "forward_lean_threshold": 0.3,
        }

        # ã‚¹ã‚±ãƒ«ãƒˆãƒ³æç”»ç”¨ã®æ¥ç¶šæƒ…å ±
        self.skeleton_connections = [
            [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],  # é¡”å‘¨ã‚Š
            [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],         # èƒ´ä½“ãƒ»è…•
            [8, 10], [9, 11], [12, 14], [13, 15], [14, 16], [15, 17]  # è…•ãƒ»è„š
        ]

        # ã‚°ãƒªãƒƒãƒ‰åˆ†å‰²è¨­å®š
        self.split_ratios = [0.5, 0.5]
        self.split_ratios_cols = [0.5, 0.5]

        # è¿½è·¡ç”¨ã®å±¥æ­´
        self.track_history = defaultdict(lambda: deque(maxlen=10))
        self.frame_count = 0

        # FPSè¨ˆç®—ç”¨
        self.fps_counter = deque(maxlen=30)  # éå»30ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¹³å‡FPSã‚’è¨ˆç®—
        self.last_frame_time = time.time()

    def _draw_skeleton(self, frame: np.ndarray, keypoints: np.ndarray, color: Tuple[int, int, int]) -> None:
        """ã‚¹ã‚±ãƒ«ãƒˆãƒ³ï¼ˆéª¨æ ¼ï¼‰ã‚’æç”»"""
        try:
            keypoints_2d = keypoints[:, :2] if keypoints.shape[1] >= 2 else keypoints

            # ã‚¹ã‚±ãƒ«ãƒˆãƒ³ç·šã‚’æç”»
            for connection in self.skeleton_connections:
                pt1_idx, pt2_idx = connection[0] - 1, connection[1] - 1  # 1-indexed to 0-indexed

                if pt1_idx < len(keypoints_2d) and pt2_idx < len(keypoints_2d):
                    pt1 = keypoints_2d[pt1_idx]
                    pt2 = keypoints_2d[pt2_idx]

                    # ä¸¡æ–¹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ç·šã‚’æç”»
                    if (pt1[0] > 0 and pt1[1] > 0 and pt2[0] > 0 and pt2[1] > 0):
                        cv2.line(frame, (int(pt1[0]), int(pt1[1])),
                                (int(pt2[0]), int(pt2[1])), color, 2)

        except Exception as e:
            logger.warning(f"ã‚¹ã‚±ãƒ«ãƒˆãƒ³æç”»ã‚¨ãƒ©ãƒ¼: {e}")

    def determine_person_orientation(self, keypoints: np.ndarray) -> PersonOrientation:
        """äººç‰©ã®å‘ãã‚’åˆ¤å®š"""
        try:
            if keypoints.shape[0] < 17:
                return PersonOrientation.UNCERTAIN

            # é¡”ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ
            nose = keypoints[0][:2]
            left_eye = keypoints[1][:2]
            right_eye = keypoints[2][:2]
            left_ear = keypoints[3][:2]
            right_ear = keypoints[4][:2]

            # è‚©ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ
            left_shoulder = keypoints[5][:2]
            right_shoulder = keypoints[6][:2]

            # å¯è¦–æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            face_points_visible = sum(
                1 for p in [nose, left_eye, right_eye, left_ear, right_ear]
                if not np.allclose(p, [0, 0])
            )

            shoulder_points_visible = sum(
                1 for p in [left_shoulder, right_shoulder]
                if not np.allclose(p, [0, 0])
            )

            # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            if face_points_visible >= 3:
                return PersonOrientation.FRONT_FACING
            elif shoulder_points_visible >= 2 and face_points_visible <= 1:
                return PersonOrientation.BACK_FACING
            elif face_points_visible >= 1 and shoulder_points_visible >= 1:
                return PersonOrientation.SIDE_FACING
            else:
                return PersonOrientation.UNCERTAIN

        except Exception as e:
            logger.error(f"å‘ãåˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return PersonOrientation.UNCERTAIN

    def extract_advanced_features(self, keypoints: np.ndarray) -> Optional[PostureFeatures]:
        """å‘ãã‚’è‡ªå‹•åˆ¤å®šã—ã¦é©åˆ‡ãªç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        orientation = self.determine_person_orientation(keypoints)

        if orientation == PersonOrientation.FRONT_FACING:
            return self._extract_features_front_view(keypoints)
        elif orientation == PersonOrientation.BACK_FACING:
            return self._extract_features_back_view(keypoints)
        else:
            features = self._extract_features_front_view(keypoints)
            if features:
                features.orientation = orientation
            return features

    def _extract_features_front_view(self, keypoints: np.ndarray) -> Optional[PostureFeatures]:
        """å‰å‘ãæ˜ åƒç”¨ã®ç‰¹å¾´é‡æŠ½å‡º"""
        try:
            if keypoints.shape[0] < 17:
                return None

            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å®šç¾©
            nose = keypoints[0][:2]
            left_ear = keypoints[3][:2]
            right_ear = keypoints[4][:2]
            left_shoulder = keypoints[5][:2]
            right_shoulder = keypoints[6][:2]
            left_elbow = keypoints[7][:2]
            right_elbow = keypoints[8][:2]
            left_wrist = keypoints[9][:2]
            right_wrist = keypoints[10][:2]

            visible_count = sum(1 for kp in keypoints if kp[0] > 0 and kp[1] > 0)

            def safe_distance(p1, p2):
                if (np.any(np.isnan(p1)) or np.any(np.isnan(p2)) or
                    np.allclose(p1, [0, 0]) or np.allclose(p2, [0, 0])):
                    return float("inf")
                return np.linalg.norm(p1 - p2)

            def calculate_angle(p1, p2, p3):
                if any(np.allclose(p, [0, 0]) for p in [p1, p2, p3]):
                    return 0.0
                v1 = p1 - p2
                v2 = p3 - p2
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                return np.degrees(np.arccos(cos_angle))

            features = PostureFeatures(
                head_angle=calculate_angle(left_ear, nose, right_ear),
                hand_face_distance_left=safe_distance(left_wrist, nose),
                hand_face_distance_right=safe_distance(right_wrist, nose),
                shoulder_hand_angle_left=calculate_angle(left_shoulder, left_elbow, left_wrist),
                shoulder_hand_angle_right=calculate_angle(right_shoulder, right_elbow, right_wrist),
                head_tilt=self._calculate_head_tilt(left_ear, right_ear),
                neck_forward=0.0,  # å‰å‘ãã§ã¯ç°¡ç•¥åŒ–
                confidence_score=np.mean(keypoints[:, 2]) if keypoints.shape[1] > 2 else 0.8,
                visible_keypoints=visible_count,
                orientation=PersonOrientation.FRONT_FACING,
            )

            return features

        except Exception as e:
            logger.error(f"å‰å‘ãç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _extract_features_back_view(self, keypoints: np.ndarray) -> Optional[PostureFeatures]:
        """å¾Œã‚å‘ãæ˜ åƒç”¨ã®ç‰¹å¾´é‡æŠ½å‡º"""
        try:
            if keypoints.shape[0] < 17:
                return None

            # å¾Œã‚ã‹ã‚‰è¦‹ãˆã‚‹ä¸»è¦ãªã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ
            left_shoulder = keypoints[5][:2]
            right_shoulder = keypoints[6][:2]
            left_elbow = keypoints[7][:2]
            right_elbow = keypoints[8][:2]
            left_wrist = keypoints[9][:2]
            right_wrist = keypoints[10][:2]

            # é ­éƒ¨ã®æ¨å®šä½ç½®
            if not (np.allclose(left_shoulder, [0, 0]) or np.allclose(right_shoulder, [0, 0])):
                shoulder_center = (left_shoulder + right_shoulder) / 2
                shoulder_width = np.linalg.norm(left_shoulder - right_shoulder)
                head_estimated = shoulder_center - np.array([0, shoulder_width * 0.3])
            else:
                head_estimated = np.array([0, 0])

            visible_count = sum(1 for kp in keypoints if kp[0] > 0 and kp[1] > 0)

            def safe_distance(p1, p2):
                if (np.any(np.isnan(p1)) or np.any(np.isnan(p2)) or 
                    np.allclose(p1, [0, 0]) or np.allclose(p2, [0, 0])):
                    return float("inf")
                return np.linalg.norm(p1 - p2)

            def calculate_angle(p1, p2, p3):
                if any(np.allclose(p, [0, 0]) for p in [p1, p2, p3]):
                    return 0.0
                v1 = p1 - p2
                v2 = p3 - p2
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                cos_angle = np.clip(cos_angle, -1.0, 1.0)
                return np.degrees(np.arccos(cos_angle))

            features = PostureFeatures(
                head_angle=self._calculate_shoulder_levelness(left_shoulder, right_shoulder),
                hand_face_distance_left=safe_distance(left_wrist, head_estimated),
                hand_face_distance_right=safe_distance(right_wrist, head_estimated),
                shoulder_hand_angle_left=calculate_angle(left_shoulder, left_elbow, left_wrist),
                shoulder_hand_angle_right=calculate_angle(right_shoulder, right_elbow, right_wrist),
                head_tilt=self._calculate_shoulder_tilt(left_shoulder, right_shoulder),
                neck_forward=0.0,  # å¾Œã‚å‘ãã§ã¯ç°¡ç•¥åŒ–
                confidence_score=np.mean(keypoints[:, 2]) if keypoints.shape[1] > 2 else 0.8,
                visible_keypoints=visible_count,
                orientation=PersonOrientation.BACK_FACING,
            )

            return features

        except Exception as e:
            logger.error(f"å¾Œã‚å‘ãç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def classify_phone_usage(self, features: PostureFeatures) -> Tuple[PhoneUsageState, float]:
        """å‘ãã«å¿œã˜ã¦ã‚¹ãƒãƒ›ä½¿ç”¨çŠ¶æ…‹ã‚’åˆ†é¡"""
        if features.orientation == PersonOrientation.FRONT_FACING:
            return self._classify_phone_usage_front_view(features)
        elif features.orientation == PersonOrientation.BACK_FACING:
            return self._classify_phone_usage_back_view(features)
        else:
            return self._classify_phone_usage_front_view(features)

    def _classify_phone_usage_front_view(self, features: PostureFeatures) -> Tuple[PhoneUsageState, float]:
        """å‰å‘ãæ˜ åƒç”¨ã®ã‚¹ãƒãƒ›ä½¿ç”¨çŠ¶æ…‹åˆ†é¡"""
        confidence = features.confidence_score

        # ä¸¡æ‰‹ãŒé¡”ã®è¿‘ãã«ã‚ã‚‹å ´åˆ
        if (features.hand_face_distance_left < self.config["phone_distance_threshold"] and
            features.hand_face_distance_right < self.config["phone_distance_threshold"]):
            return PhoneUsageState.BOTH_HANDS_UP, confidence * 0.9

        # ç‰‡æ‰‹ãŒé¡”ã®è¿‘ãã«ã‚ã‚‹å ´åˆ
        elif (features.hand_face_distance_left < self.config["phone_distance_threshold"] or
            features.hand_face_distance_right < self.config["phone_distance_threshold"]):
            if features.head_tilt > self.config["head_angle_threshold"]:
                return PhoneUsageState.LOOKING_DOWN, confidence * 0.8
            else:
                return PhoneUsageState.HOLDING_NEAR_FACE, confidence * 0.85

        # åˆ¤å®šå›°é›£ãªå ´åˆ
        elif features.visible_keypoints < 10:
            return PhoneUsageState.UNCERTAIN, confidence * 0.5
        else:
            return PhoneUsageState.NOT_USING, confidence * 0.9

    def _classify_phone_usage_back_view(self, features: PostureFeatures) -> Tuple[PhoneUsageState, float]:
        """å¾Œã‚å‘ãæ˜ åƒç”¨ã®ã‚¹ãƒãƒ›ä½¿ç”¨çŠ¶æ…‹åˆ†é¡"""
        confidence = features.confidence_score

        # ä¸¡æ‰‹ãŒé ­éƒ¨ä»˜è¿‘ã«ã‚ã‚‹å ´åˆ
        if (features.hand_face_distance_left < self.back_view_config["hand_head_distance_threshold"] and
            features.hand_face_distance_right < self.back_view_config["hand_head_distance_threshold"]):
            return PhoneUsageState.BOTH_HANDS_UP, confidence * 0.8

        # ç‰‡æ‰‹ãŒé ­éƒ¨ä»˜è¿‘ã«ã‚ã‚‹å ´åˆ
        elif (features.hand_face_distance_left < self.back_view_config["hand_head_distance_threshold"] or
            features.hand_face_distance_right < self.back_view_config["hand_head_distance_threshold"]):
            if (features.shoulder_hand_angle_left < self.back_view_config["arm_bend_threshold"] or
                features.shoulder_hand_angle_right < self.back_view_config["arm_bend_threshold"]):
                return PhoneUsageState.HOLDING_NEAR_FACE, confidence * 0.7
            else:
                return PhoneUsageState.UNCERTAIN, confidence * 0.5

        # åˆ¤å®šå›°é›£ãªå ´åˆ
        elif features.visible_keypoints < 8:
            return PhoneUsageState.UNCERTAIN, confidence * 0.4
        else:
            return PhoneUsageState.NOT_USING, confidence * 0.8

    def _calculate_head_tilt(self, left_ear: np.ndarray, right_ear: np.ndarray) -> float:
        """é ­éƒ¨ã®å‚¾ãã‚’è¨ˆç®—"""
        if np.allclose(left_ear, [0, 0]) or np.allclose(right_ear, [0, 0]):
            return 0.0

        height_diff = abs(left_ear[1] - right_ear[1])
        width_diff = abs(left_ear[0] - right_ear[0])

        if width_diff == 0:
            return 0.0

        return np.degrees(np.arctan(height_diff / width_diff))

    def _calculate_shoulder_levelness(self, left_shoulder: np.ndarray, right_shoulder: np.ndarray) -> float:
        """è‚©ã®æ°´å¹³åº¦ã‚’è¨ˆç®—"""
        if np.allclose(left_shoulder, [0, 0]) or np.allclose(right_shoulder, [0, 0]):
            return 0.0

        shoulder_vector = right_shoulder - left_shoulder
        horizontal_vector = np.array([1, 0])

        cos_angle = np.dot(shoulder_vector, horizontal_vector) / np.linalg.norm(shoulder_vector)
        cos_angle = np.clip(cos_angle, -1.0, 1.0)
        return np.degrees(np.arccos(abs(cos_angle)))

    def _calculate_shoulder_tilt(self, left_shoulder: np.ndarray, right_shoulder: np.ndarray) -> float:
        """è‚©ã®å‚¾ãã‚’è¨ˆç®—"""
        if np.allclose(left_shoulder, [0, 0]) or np.allclose(right_shoulder, [0, 0]):
            return 0.0

        height_diff = abs(left_shoulder[1] - right_shoulder[1])
        width_diff = abs(left_shoulder[0] - right_shoulder[0])

        if width_diff == 0:
            return 0.0

        return np.degrees(np.arctan(height_diff / width_diff))

    def smooth_detection(self, track_id, phone_state):
        """æ¤œå‡ºçµæœã‚’å¹³æ»‘åŒ–ï¼ˆtrack_idãƒ™ãƒ¼ã‚¹ï¼‰"""
        if track_id not in self.person_states:
            self.person_states[track_id] = {
                "phone_history": deque(maxlen=5),
                "last_seen": 0,
            }

        state = self.person_states[track_id]
        state["phone_history"].append(phone_state)

        # éåŠæ•°ã®åˆ¤å®šã§æ±ºå®š
        if len(state["phone_history"]) == 0:
            return PhoneUsageState.NOT_USING

        # æœ€ã‚‚å¤šã„çŠ¶æ…‹ã‚’æ¡ç”¨
        state_counts = defaultdict(int)
        for s in state["phone_history"]:
            state_counts[s] += 1

        return max(state_counts, key=state_counts.get)

    def draw_monitor_grid(self, img, col_ratios, row_ratios):
        """ç›£è¦–ã‚°ãƒªãƒƒãƒ‰ã‚’æç”»"""
        h, w = img.shape[:2]
        # ç¸¦ç·š
        x_current = 0
        for ratio in col_ratios[:-1]:
            x_current += int(w * ratio)
            cv2.line(img, (x_current, 0), (x_current, h), (255, 255, 255), 2)
        # æ¨ªç·š
        y_current = 0
        for ratio in row_ratios[:-1]:
            y_current += int(h * ratio)
            cv2.line(img, (0, y_current), (w, y_current), (255, 255, 255), 2)

    def get_grid_position(self, bbox: Tuple[int, int, int, int], frame_width: int, frame_height: int) -> Tuple[int, int]:
        """äººç‰©ã®ä½ç½®ã‚’ã‚°ãƒªãƒƒãƒ‰åº§æ¨™ã§å–å¾—"""
        x1, y1, x2, y2 = bbox
        center_x = (x1 + x2) // 2
        center_y = (y1 + y2) // 2

        grid_x = 0 if center_x < frame_width * self.split_ratios_cols[0] else 1
        grid_y = 0 if center_y < frame_height * self.split_ratios[0] else 1

        return (grid_y, grid_x)

    def process_frame(self, frame, frame_idx, csv_writer, enhanced_csv_logger=None):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã¦æ¤œå‡ºçµæœã‚’è¿”ã™ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        self.frame_count = frame_idx
        height, width = frame.shape[:2]

        # YOLOæ¤œå‡ºå®Ÿè¡Œ
        try:
            results = self.model(frame, conf=self.config["conf_threshold"], verbose=False)
        except Exception as e:
            logger.error(f"YOLOæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return frame

        # ã‚°ãƒªãƒƒãƒ‰å¢ƒç•Œè¨ˆç®—
        x_grid = [0, int(width * self.split_ratios_cols[0]), width]
        y_grid = [0, int(height * self.split_ratios[0]), height]

        # æ¤œå‡ºçµæœã‚’æ•´ç†
        detections = []
        for result in results:
            if result.keypoints is None or result.boxes is None:
                continue

            try:
                kps_list = result.keypoints.xy.cpu().numpy()
                boxes = result.boxes.xyxy.cpu().numpy()
                confs = result.boxes.conf.cpu().numpy()

                for kps, box, conf in zip(kps_list, boxes, confs):
                    if conf < self.config["conf_threshold"]:
                        continue

                    if kps.shape[0] < 17:
                        continue

                    # ä¸­å¿ƒåº§æ¨™è¨ˆç®—
                    valid_points = kps[kps[:, 0] > 0]
                    if len(valid_points) == 0:
                        continue
                    center = np.mean(valid_points[:, :2], axis=0)
                    cx, cy = int(center[0]), int(center[1])

                    detections.append({
                        'center': (cx, cy),
                        'bbox': box,
                        'keypoints': kps,
                        'confidence': conf
                    })

            except Exception as e:
                logger.error(f"æ¤œå‡ºçµæœå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # é †åºä»˜ãIDãƒˆãƒ©ãƒƒã‚«ãƒ¼ã§è¿½è·¡æ›´æ–°
        tracked_detections = self.id_tracker.update_tracks(detections)

        # ã‚°ãƒªãƒƒãƒ‰æç”»
        self.draw_monitor_grid(frame, self.split_ratios_cols, self.split_ratios)

        # æ¤œå‡ºçµæœã®å‡¦ç†
        detection_results = []
        for detection in tracked_detections:
            track_id = detection['track_id']
            kps = detection['keypoints']
            box = detection['bbox']
            cx, cy = detection['center']

            # é«˜åº¦ãªç‰¹å¾´é‡æŠ½å‡º
            features = self.extract_advanced_features(kps)
            if features is None:
                continue

            # ã‚¹ãƒãƒ›ä½¿ç”¨çŠ¶æ…‹åˆ†é¡
            phone_state_raw, state_confidence = self.classify_phone_usage(features)
            phone_state = self.smooth_detection(track_id, phone_state_raw)

            # ã‚°ãƒªãƒƒãƒ‰ä½ç½®
            bbox_tuple = tuple(map(int, box))
            grid_pos = self.get_grid_position(bbox_tuple, width, height)
            row, col = grid_pos

            # DetectionResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            detection_result = DetectionResult(
                frame_id=frame_idx,
                track_id=track_id,
                timestamp=time.time(),
                phone_state=phone_state,
                confidence=state_confidence,
                features=features,
                bbox=bbox_tuple,
                grid_position=grid_pos,
                keypoints_visible=[kpt[0] > 0 and kpt[1] > 0 for kpt in kps],
                orientation=features.orientation
            )

            detection_results.append(detection_result)

            # åŸºæœ¬CSVã«çµæœã‚’è¨˜éŒ²
            if csv_writer:
                # å¾“æ¥å½¢å¼ã«åˆã‚ã›ã¦ç°¡ç•¥åŒ–ã—ãŸå€¤ã‚’è¨˜éŒ²
                using_phone = phone_state not in [PhoneUsageState.NOT_USING, PhoneUsageState.UNCERTAIN]
                csv_writer.writerow([frame_idx, track_id, using_phone, row, col])

            # æ‹¡å¼µCSVã«çµæœã‚’è¨˜éŒ²
            if enhanced_csv_logger is not None:
                try:
                    enhanced_csv_logger.log_detection_result(
                        detection_result, frame, kps, row, col, "google_drive"
                    )
                except Exception as csv_error:
                    logger.error(f"æ‹¡å¼µCSVè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {csv_error}")

            # æç”»å‡¦ç†
            self._draw_detection_on_frame(frame, detection_result, kps)

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
        active_tracks = self.id_tracker.get_active_tracks()
        active_ids = sorted(active_tracks.keys())
        id_info = f"Active IDs (Lâ†’R): {active_ids}" if active_ids else "Active IDs: None"

        cv2.putText(frame, id_info, (20, height - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        cv2.putText(frame, f"Total Persons: {len(active_ids)}", (20, height - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        self._draw_fps(frame)
        return frame

    def _calculate_fps(self) -> float:
        """ç¾åœ¨ã®FPSã‚’è¨ˆç®—"""
        current_time = time.time()
        frame_time = current_time - self.last_frame_time
        self.last_frame_time = current_time

        if frame_time > 0:
            fps = 1.0 / frame_time
            self.fps_counter.append(fps)

        # éå»æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¹³å‡FPSã‚’è¿”ã™
        if len(self.fps_counter) > 0:
            return sum(self.fps_counter) / len(self.fps_counter)
        return 0.0

    def _draw_fps(self, frame: np.ndarray) -> None:
        """FPSæƒ…å ±ã‚’ç”»é¢ã«æç”»"""
        current_fps = self._calculate_fps()

        # FPSè¡¨ç¤ºä½ç½®ï¼ˆå³ä¸Šè§’ï¼‰
        height, width = frame.shape[:2]
        fps_text = f"FPS: {current_fps:.1f}"

        # ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’æ¸¬å®š
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.8
        thickness = 2
        (text_width, text_height), baseline = cv2.getTextSize(fps_text, font, font_scale, thickness)

        # èƒŒæ™¯çŸ©å½¢ã‚’æç”»ï¼ˆè¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰
        bg_x1 = width - text_width - 20
        bg_y1 = 10
        bg_x2 = width - 10
        bg_y2 = text_height + baseline + 20

        cv2.rectangle(frame, (bg_x1, bg_y1), (bg_x2, bg_y2), (0, 0, 0), -1)  # é»’èƒŒæ™¯
        cv2.rectangle(frame, (bg_x1, bg_y1), (bg_x2, bg_y2), (255, 255, 255), 1)  # ç™½æ 

        # FPSãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»
        cv2.putText(frame, fps_text, (bg_x1 + 10, bg_y1 + text_height + 5),
                font, font_scale, (0, 255, 0), thickness)

    def _draw_detection_on_frame(self, frame, detection_result: DetectionResult, keypoints):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ¤œå‡ºçµæœã‚’æç”»"""
        x1, y1, x2, y2 = detection_result.bbox

        # çŠ¶æ…‹ã«å¿œã˜ã¦è‰²ã‚’è¨­å®š
        color_map = {
            PhoneUsageState.NOT_USING: (0, 255, 0),           # ç·‘
            PhoneUsageState.HOLDING_NEAR_FACE: (0, 165, 255), # ã‚ªãƒ¬ãƒ³ã‚¸
            PhoneUsageState.LOOKING_DOWN: (0, 0, 255),        # èµ¤
            PhoneUsageState.BOTH_HANDS_UP: (255, 0, 255),     # ãƒã‚¼ãƒ³ã‚¿
            PhoneUsageState.UNCERTAIN: (128, 128, 128),       # ã‚°ãƒ¬ãƒ¼
            PhoneUsageState.TRANSITIONING: (255, 255, 0),     # ã‚·ã‚¢ãƒ³
        }

        color = color_map.get(detection_result.phone_state, (255, 255, 255))

        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æç”»
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)

        # ãƒ©ãƒ™ãƒ«ä½œæˆ
        state_names = {
            PhoneUsageState.NOT_USING: "Awake",
            PhoneUsageState.HOLDING_NEAR_FACE: "Phone",
            PhoneUsageState.LOOKING_DOWN: "LookDown",
            PhoneUsageState.BOTH_HANDS_UP: "BothHands",
            PhoneUsageState.UNCERTAIN: "Uncertain",
            PhoneUsageState.TRANSITIONING: "Transit"
        }

        orientation_short = {
            PersonOrientation.FRONT_FACING: "F",
            PersonOrientation.BACK_FACING: "B",
            PersonOrientation.SIDE_FACING: "S",
            PersonOrientation.UNCERTAIN: "?"
        }

        label = f"ID:{detection_result.track_id} {state_names[detection_result.phone_state]}"
        label += f" [{orientation_short[detection_result.orientation]}]"
        label += f" [R{detection_result.grid_position[0]},C{detection_result.grid_position[1]}]"

        # ãƒ©ãƒ™ãƒ«æç”»
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

        # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæç”»
        for pt in keypoints.astype(int):
            if len(pt) >= 2 and pt[0] > 0 and pt[1] > 0:
                cv2.circle(frame, tuple(pt[:2]), 3, (255, 255, 0), -1)

        # ã‚¹ã‚±ãƒ«ãƒˆãƒ³æç”»
        self._draw_skeleton(frame, keypoints, color)

class IntegratedVideoProcessor:
    """çµ±åˆå‹•ç”»å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""

    def __init__(self, k1=-0.1, k2=0.0, p1=0.0, p2=0.0, k3=0.0, alpha=0.6, focal_scale=0.9, model_path="yolo11x-pose.pt"):
        # æ”¹è‰¯ç‰ˆæ­ªã¿è£œæ­£å™¨
        self.corrector = VideoDistortionCorrector(k1, k2, p1, p2, k3, alpha, focal_scale)
        # é«˜åº¦ãªå§¿å‹¢æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
        self.detector = AdvancedPostureDetectionSystem(model_path)
        # æ‹¡å¼µCSVãƒ­ã‚¬ãƒ¼
        self.csv_logger = None

    def set_csv_logger(self, csv_path="enhanced_detection_log.csv"):
        """CSVãƒ­ã‚¬ãƒ¼ã‚’è¨­å®š"""
        try:
            self.csv_logger = EnhancedCSVLogger(csv_path)
            logger.info(f"æ‹¡å¼µCSVãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–æˆåŠŸ: {csv_path}")
        except Exception as e:
            logger.error(f"æ‹¡å¼µCSVãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–å¤±æ•—: {e}")
            self.csv_logger = None

    def process_video(self, input_path, output_path, result_log="frame_results.csv",
                    show_preview=True, apply_correction=True):
        """å‹•ç”»ã‚’å‡¦ç†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            logger.error(f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“: {input_path}")
            return

        # å‹•ç”»æƒ…å ±å–å¾—
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        logger.info(f"å‹•ç”»æƒ…å ±: {width}x{height}, {fps:.1f}fps, {total_frames}ãƒ•ãƒ¬ãƒ¼ãƒ ")

        # æ­ªã¿è£œæ­£ãƒãƒƒãƒ—ä½œæˆ
        if apply_correction:
            logger.info("é«˜ç²¾åº¦æ­ªã¿è£œæ­£ãƒãƒƒãƒ—ã‚’ä½œæˆä¸­...")
            self.corrector.create_correction_maps(width, height)

        # å‡ºåŠ›å‹•ç”»è¨­å®š
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # çµæœãƒ­ã‚°æº–å‚™
        os.makedirs(os.path.dirname(result_log), exist_ok=True)
        detection_count = 0

        with open(result_log, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["frame", "person_id", "using_phone", "grid_row", "grid_col"])

            frame_idx = 0
            start_time = time.time()
            all_detection_results = []

            logger.info("æ”¹è‰¯ç‰ˆå‹•ç”»å‡¦ç†ã‚’é–‹å§‹...")

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                frame_idx += 1

                # æ­ªã¿è£œæ­£é©ç”¨
                if apply_correction:
                    frame = self.corrector.apply_correction(frame)

                # å§¿å‹¢æ¤œå‡ºå‡¦ç†
                try:
                    frame = self.detector.process_frame(
                        frame, frame_idx, writer, self.csv_logger
                    )

                    # æ¤œå‡ºçµæœã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    if self.csv_logger and hasattr(self.csv_logger, 'log_count'):
                        detection_count = self.csv_logger.log_count

                except Exception as detection_error:
                    logger.error(f"ãƒ•ãƒ¬ãƒ¼ãƒ {frame_idx}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {detection_error}")

                # ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±è¡¨ç¤º
                cv2.putText(frame, f"Frame: {frame_idx}/{total_frames}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)

                # æ‹¡å¼µCSVè¨˜éŒ²çŠ¶æ³è¡¨ç¤º
                if self.csv_logger:
                    cv2.putText(frame, f"Enhanced CSV: {detection_count} records", (20, 80),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
                if show_preview:
                    cv2.imshow("Advanced Posture Detection System", frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        logger.info("å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                        break

                # çµæœä¿å­˜
                out.write(frame)

                # é€²è¡ŒçŠ¶æ³è¡¨ç¤º
                if frame_idx % 30 == 0:
                    elapsed = time.time() - start_time
                    fps_current = frame_idx / elapsed
                    progress = (frame_idx / total_frames) * 100
                    eta = (total_frames - frame_idx) / fps_current if fps_current > 0 else 0

                    active_tracks = self.detector.id_tracker.get_active_tracks()
                    logger.info(
                        f"é€²è¡ŒçŠ¶æ³: {progress:.1f}% ({frame_idx}/{total_frames}) "
                        f"å‡¦ç†é€Ÿåº¦: {fps_current:.1f}fps æ®‹ã‚Š: {eta:.1f}s "
                        f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ID: {sorted(active_tracks.keys())} "
                        f"æ‹¡å¼µCSV: {detection_count}è¡Œ"
                    )

        # ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
        cap.release()
        out.release()
        cv2.destroyAllWindows()

        # CSVãƒ­ã‚¬ãƒ¼ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
        if self.csv_logger:
            self.csv_logger.close()

        # å®Œäº†å ±å‘Š
        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ æ”¹è‰¯ç‰ˆå‹•ç”»å‡¦ç†å®Œäº†!")
        logger.info(f"å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        logger.info(f"å¹³å‡å‡¦ç†é€Ÿåº¦: {frame_idx/total_time:.1f}fps")
        logger.info(f"æœ€çµ‚æ‹¡å¼µCSVè¨˜éŒ²æ•°: {detection_count}è¡Œ")

    def get_statistics(self) -> Dict:
        """å‡¦ç†çµ±è¨ˆã‚’å–å¾—"""
        active_tracks = self.detector.id_tracker.get_active_tracks()
        return {
            'active_tracks': len(active_tracks),
            'total_csv_records': self.csv_logger.log_count if self.csv_logger else 0,
            'track_ids': sorted(active_tracks.keys())
        }